# Welcome to ExLlamaV2

ExLlamaV2 is a state-of-the-art inference engine that enables running sophisticated models like Llama3, Mistral, and Qwen directly on personal hardware. By leveraging advanced optimization techniques, tensor processing, and flexible quantization, ExLlamaV2 allows users to run large-scale models with minimal hardware requirements.

## Why Choose ExLlamaV2?

- **Blazing-Fast Performance**: Achieve high token generation speeds with paged attention (via Flash Attention), smart prompt caching, and K/V cache deduplication.
- **Advanced Quantization**: Supports GPTQ and EXL2 formats, allowing dynamic 2-8 bit quantization for optimal balance between speed and memory efficiency.
- **Privacy-Focused**: Run everything locally, eliminating concerns about data privacy or reliance on third-party servers.
- **Cost-Effective**: Avoid recurring cloud service costs by running models on your own hardware.
- **Seamless Integration**: Works with popular tools like TabbyAPI, ExUI, and text-generation-webui for an effortless setup.

### Key Features

- **Dynamic Batching**: Efficiently processes multiple requests simultaneously for enhanced throughput.
- **Tensor Processing**: Optimized tensor operations leveraging CUDA for high-speed computations.
- **Smart Prompt Caching**: Minimizes redundant processing by reusing previous computations.
- **Flexible Quantization**: Fine-tune precision levels to optimize performance and memory usage.
- **Comprehensive Model Support**: Compatible with a wide range of Llama-based and transformer models.
- **Active Community**: Get support, share insights, and contribute to continuous improvements.


## Getting Started

Join our vibrant [Discord Community](https://discord.gg/dsCjen9AUV) to get assistance, share experiences, and collaborate with fellow ExLlamaV2 users. Our active support network ensures you get the most out of your models.

[Installation Guide →](link-to-installation-guide)  
[Quick Start Tutorial →](link-to-getting-started)

### Contributing

ExLlamaV2 is an open-source project driven by its community. We welcome contributions—whether it's fixing bugs, adding features, or improving documentation, your involvement makes a difference.

[Contribution Guidelines →](link-to-contribution-guide)

### Next Steps

- [Complete Installation Guide](link-to-installation)
- [Running Your First Model](link-to-first-model)
- [Advanced Configuration](link-to-advanced-config)
- [API Reference](link-to-api-reference)
- [Model Compatibility](link-to-compatibility)

---

Ready to run powerful language models on your own hardware? Get started with ExLlamaV2 today!

[⭐ Star us on GitHub](https://github.com/turboderp-org/exllamav2) | [Join Discord](https://discord.gg/dsCjen9AUV) | [Latest Release](https://github.com/turboderp-org/exllamav2/releases)

